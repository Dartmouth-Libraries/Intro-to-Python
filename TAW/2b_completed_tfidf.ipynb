{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text by the Numbers: Word Vectors\n",
    "\n",
    "**A Reproducible Research Workshop**\n",
    "\n",
    "(A Collaboration between Dartmouth Library and Research Computing)\n",
    "\n",
    "[*Click here to view or register for our current list of workshops*](http://dartgo.org/RRADworkshops)\n",
    "\n",
    "*This notebook created by*:\n",
    "+ Version 1.0: Jeremy Mikecz, Research Data Services (Dartmouth Library)\n",
    "+ Version 2.0: ???\n",
    "<!--\n",
    "+ Some of the inspiration for the code and information in this notebook was taken from https://www.w3schools.com/python/python_intro.asp -- This is a great resource if you want to learn more about Python!-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Word Vectors\n",
    "\n",
    "[types of vectorization]\n",
    "\n",
    "[what can we learn?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II: Setup\n",
    "\n",
    "1. Before beginning, we need to import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import glob \n",
    "import pandas as pd\n",
    "\n",
    "textdir = Path(\"~/shared/RR-workshop-data/state-of-the-union-dataset/txt\").expanduser() \n",
    "pathlist = sorted(textdir.glob('*.txt')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Data Frequency (TFIDF)\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/720/1*qQgnyPLDIkUmeZKN2_ZWbQ.webp\" style=\"width:60%\">\n",
    "\n",
    "Image from Yassine Hamdaoui, [\"TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python\"](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558) *Towards Data Science (Medium)* (Dec. 9, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. TF-IDF with Scikit-Learn [MW]\n",
    "\n",
    "Tf-idf is a method that tries to identify the most distinctively frequent or significant words in a document. \n",
    "\n",
    "In this lesson, we’re going to learn how to calculate tf-idf scores using a collection of plain text (.txt) files and the Python library scikit-learn, which has a quick and nifty module called TfidfVectorizer.\n",
    "\n",
    "In this lesson, we will cover how to:\n",
    "\n",
    "    Calculate and normalize tf-idf scores for U.S. Inaugural Addresses with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Breaking Down the TF-IDF Formula [MW]\n",
    "\n",
    "But first, let’s quickly discuss the tf-idf formula. The idea is pretty simple.\n",
    "\n",
    "**tf-idf = term_frequency * inverse_document_frequency**\n",
    "\n",
    "**term_frequency** = number of times a given term appears in document\n",
    "\n",
    "**inverse_document_frequency** = log(total number of documents / number of documents with term) + 1*****\n",
    "\n",
    "You take the number of times a term occurs in a document (term frequency). Then you take the number of documents in which the same term occurs at least once divided by the total number of documents (document frequency), and you flip that fraction on its head (inverse document frequency). Then you multiply the two numbers together (term_frequency * inverse_document_frequency).\n",
    "\n",
    "The reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents. Think about the inverse document frequency for the word “said” vs the word “pigeon.” The term “said” appears in 13 (document frequency) of 24 (total documents) Lost in the City stories (24 / 13 –> a smaller inverse document frequency) while the term “pigeons” only occurs in 2 (document frequency) of the 24 stories (total documents) (24 / 2 –> a bigger inverse document frequency, a bigger tf-idf boost).\n",
    "\n",
    "*There are a bunch of slightly different ways that you can calculate inverse document frequency. The version of idf that we’re going to use is the scikit-learn default, which uses “smoothing” aka it adds a “1” to the numerator and denominator:\n",
    "\n",
    "**inverse_document_frequency** = log((1 + total_number_of_documents) / (number_of_documents_with_term +1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Calculate tf-idf [MW]\n",
    "\n",
    "To calculate tf–idf scores for every word, we’re going to use scikit-learn’s TfidfVectorizer.\n",
    "\n",
    "4. When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
    "\n",
    "The recommended way to run TfidfVectorizer is with smoothing (smooth_idf = True) and normalization (norm='l2') turned on. These parameters will better account for differences in text length, and overall produce more meaningful tf–idf scores. Smoothing and L2 normalization are actually the default settings for TfidfVectorizer, so to turn them on, you don’t need to include any extra code at all.\n",
    "\n",
    "Initialize TfidfVectorizer with desired parameters (default smoothing and normalization).\n",
    "\n",
    "**Note: tfidf vectors can become very large even for a modest number of texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
    "tfidf_vectorizer2 = TfidfVectorizer(input='filename', stop_words='english', max_df = 0.5, max_features=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run TfidfVectorizer on our text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<233x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 207870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector = tfidf_vectorizer2.fit_transform(pathlist)\n",
    "tfidf_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a DataFrame out of the resulting tf–idf vector, setting the “feature names” or words as columns and the titles as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              00   08  100      10th   11  112  11th        12  120  125  ...  \\\n",
      "Adams_1797   0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Adams_1798   0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Adams_1799   0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Adams_1800   0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Adams_1825   0.0  0.0  0.0  0.046041  0.0  0.0   0.0  0.011819  0.0  0.0  ...   \n",
      "...          ...  ...  ...       ...  ...  ...   ...       ...  ...  ...  ...   \n",
      "Wilson_1916  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Wilson_1917  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Wilson_1918  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Wilson_1919  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "Wilson_1920  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
      "\n",
      "                yield  yielded  yielding  york  young  younger  youth  \\\n",
      "Adams_1797   0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Adams_1798   0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Adams_1799   0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Adams_1800   0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Adams_1825   0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "...               ...      ...       ...   ...    ...      ...    ...   \n",
      "Wilson_1916  0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Wilson_1917  0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Wilson_1918  0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Wilson_1919  0.020876      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "Wilson_1920  0.000000      0.0       0.0   0.0    0.0      0.0    0.0   \n",
      "\n",
      "                 zeal  zone  zones  \n",
      "Adams_1797   0.000000   0.0    0.0  \n",
      "Adams_1798   0.000000   0.0    0.0  \n",
      "Adams_1799   0.045193   0.0    0.0  \n",
      "Adams_1800   0.000000   0.0    0.0  \n",
      "Adams_1825   0.000000   0.0    0.0  \n",
      "...               ...   ...    ...  \n",
      "Wilson_1916  0.000000   0.0    0.0  \n",
      "Wilson_1917  0.026059   0.0    0.0  \n",
      "Wilson_1918  0.022026   0.0    0.0  \n",
      "Wilson_1919  0.000000   0.0    0.0  \n",
      "Wilson_1920  0.000000   0.0    0.0  \n",
      "\n",
      "[233 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "text_titles = [path.stem for path in pathlist]\n",
    "#TfidfVectorizer returns a sparse matrix and that's why we have to call .toarray()  before proceeding.\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer2.get_feature_names_out())\n",
    "#warning: get_feature_names will be depreciated; use get_feature_names_out instead\n",
    "   ##I made this fix in the code above\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>08</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>112</th>\n",
       "      <th>11th</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>125</th>\n",
       "      <th>...</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yielding</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adams_1797</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1798</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1799</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             00   08  100      10th   11  112  11th        12  120  125  ...  \\\n",
       "Adams_1797  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
       "Adams_1798  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
       "Adams_1799  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
       "Adams_1800  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  0.0  ...   \n",
       "Adams_1825  0.0  0.0  0.0  0.046041  0.0  0.0   0.0  0.011819  0.0  0.0  ...   \n",
       "\n",
       "            yield  yielded  yielding  york  young  younger  youth      zeal  \\\n",
       "Adams_1797    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   \n",
       "Adams_1798    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   \n",
       "Adams_1799    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.045193   \n",
       "Adams_1800    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   \n",
       "Adams_1825    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   \n",
       "\n",
       "            zone  zones  \n",
       "Adams_1797   0.0    0.0  \n",
       "Adams_1798   0.0    0.0  \n",
       "Adams_1799   0.0    0.0  \n",
       "Adams_1800   0.0    0.0  \n",
       "Adams_1825   0.0    0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>00</th>\n",
       "      <th>08</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>112</th>\n",
       "      <th>11th</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>...</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yielding</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams_1797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adams_1798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adams_1799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adams_1825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     textname   00   08  100      10th   11  112  11th        12  120  ...  \\\n",
       "0  Adams_1797  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  ...   \n",
       "1  Adams_1798  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  ...   \n",
       "2  Adams_1799  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  ...   \n",
       "3  Adams_1800  0.0  0.0  0.0  0.000000  0.0  0.0   0.0  0.000000  0.0  ...   \n",
       "4  Adams_1825  0.0  0.0  0.0  0.046041  0.0  0.0   0.0  0.011819  0.0  ...   \n",
       "\n",
       "   yield  yielded  yielding  york  young  younger  youth      zeal  zone  \\\n",
       "0    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   0.0   \n",
       "1    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   0.0   \n",
       "2    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.045193   0.0   \n",
       "3    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   0.0   \n",
       "4    0.0      0.0       0.0   0.0    0.0      0.0    0.0  0.000000   0.0   \n",
       "\n",
       "   zones  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 5001 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.index.name = \"textname\"\n",
    "tfidf_df = tfidf_df.reset_index()\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams_1797</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adams_1798</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adams_1799</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adams_1825</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     textname word  tfidf_score\n",
       "0  Adams_1797   00          0.0\n",
       "1  Adams_1798   00          0.0\n",
       "2  Adams_1799   00          0.0\n",
       "3  Adams_1800   00          0.0\n",
       "4  Adams_1825   00          0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_long =  pd.melt(tfidf_df, id_vars = \"textname\", var_name = \"word\", value_name = \"tfidf_score\", value_vars = list(tfidf_df.drop(columns = [\"textname\"]).columns))\n",
    "tfidf_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1165000, 3)\n",
      "(207870, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_long.shape)\n",
    "tfidf_long = tfidf_long[tfidf_long['tfidf_score'] > 0.0]\n",
    "print(tfidf_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207870, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taft_1910</td>\n",
       "      <td>00</td>\n",
       "      <td>0.790884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson_1966</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.663404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arthur_1883</td>\n",
       "      <td>00</td>\n",
       "      <td>0.618767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carter_1980</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.607743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arthur_1882</td>\n",
       "      <td>00</td>\n",
       "      <td>0.595908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>gold</td>\n",
       "      <td>0.563849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.554145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eisenhower_1961</td>\n",
       "      <td>1953</td>\n",
       "      <td>0.532740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polk_1847</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.532682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Harrison_1892</td>\n",
       "      <td>1892</td>\n",
       "      <td>0.514329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>gentlemen</td>\n",
       "      <td>0.476052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Clinton_1996</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.472962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Truman_1946</td>\n",
       "      <td>1947</td>\n",
       "      <td>0.462230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Roosevelt_1943</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.460881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Monroe_1819</td>\n",
       "      <td>spain</td>\n",
       "      <td>0.454886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>texas</td>\n",
       "      <td>0.442920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Adams_1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>0.441875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hoover_1930</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.440265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Truman_1951</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.437807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Johnson_1967</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.433012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Arthur_1881</td>\n",
       "      <td>00</td>\n",
       "      <td>0.432138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Trump_2020</td>\n",
       "      <td>thank</td>\n",
       "      <td>0.429942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Roosevelt_1937</td>\n",
       "      <td>democracy</td>\n",
       "      <td>0.425793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Johnson_1968</td>\n",
       "      <td>billion</td>\n",
       "      <td>0.425429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Eisenhower_1956</td>\n",
       "      <td>program</td>\n",
       "      <td>0.417399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Madison_1814</td>\n",
       "      <td>enemy</td>\n",
       "      <td>0.414576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Monroe_1818</td>\n",
       "      <td>spain</td>\n",
       "      <td>0.412596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hoover_1931</td>\n",
       "      <td>banks</td>\n",
       "      <td>0.409236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Carter_1978</td>\n",
       "      <td>ve</td>\n",
       "      <td>0.407975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Buren_1839</td>\n",
       "      <td>banks</td>\n",
       "      <td>0.406933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>hussein</td>\n",
       "      <td>0.406446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>saddam</td>\n",
       "      <td>0.406446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ford_1975</td>\n",
       "      <td>oil</td>\n",
       "      <td>0.403722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Biden_2021</td>\n",
       "      <td>jobs</td>\n",
       "      <td>0.402133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Roosevelt_1903</td>\n",
       "      <td>isthmus</td>\n",
       "      <td>0.397411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Cleveland_1888</td>\n",
       "      <td>1888</td>\n",
       "      <td>0.397201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Trump_2018</td>\n",
       "      <td>tonight</td>\n",
       "      <td>0.397188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Hayes_1877</td>\n",
       "      <td>coinage</td>\n",
       "      <td>0.392970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Bush_2007</td>\n",
       "      <td>iraq</td>\n",
       "      <td>0.392226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Bush_2008</td>\n",
       "      <td>iraq</td>\n",
       "      <td>0.389564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Cleveland_1887</td>\n",
       "      <td>tariff</td>\n",
       "      <td>0.387655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Tyler_1844</td>\n",
       "      <td>texas</td>\n",
       "      <td>0.384964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Obama_2015</td>\n",
       "      <td>ve</td>\n",
       "      <td>0.379427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Truman_1953</td>\n",
       "      <td>communist</td>\n",
       "      <td>0.378838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Bush_2001</td>\n",
       "      <td>budget</td>\n",
       "      <td>0.378685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Trump_2019</td>\n",
       "      <td>thank</td>\n",
       "      <td>0.377305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Bush_1989</td>\n",
       "      <td>tonight</td>\n",
       "      <td>0.372753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Reagan_1982</td>\n",
       "      <td>programs</td>\n",
       "      <td>0.366303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Truman_1953</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.366231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Cleveland_1893</td>\n",
       "      <td>1893</td>\n",
       "      <td>0.365565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textname       word  tfidf_score\n",
       "0         Taft_1910         00     0.790884\n",
       "1      Johnson_1966    vietnam     0.663404\n",
       "2       Arthur_1883         00     0.618767\n",
       "3       Carter_1980     soviet     0.607743\n",
       "4       Arthur_1882         00     0.595908\n",
       "5    Cleveland_1895       gold     0.563849\n",
       "6         Polk_1846     mexico     0.554145\n",
       "7   Eisenhower_1961       1953     0.532740\n",
       "8         Polk_1847     mexico     0.532682\n",
       "9     Harrison_1892       1892     0.514329\n",
       "10       Adams_1800  gentlemen     0.476052\n",
       "11     Clinton_1996  challenge     0.472962\n",
       "12      Truman_1946       1947     0.462230\n",
       "13   Roosevelt_1943       1942     0.460881\n",
       "14      Monroe_1819      spain     0.454886\n",
       "15        Polk_1846      texas     0.442920\n",
       "16       Adams_1827       1827     0.441875\n",
       "17      Hoover_1930       1928     0.440265\n",
       "18      Truman_1951     soviet     0.437807\n",
       "19     Johnson_1967    vietnam     0.433012\n",
       "20      Arthur_1881         00     0.432138\n",
       "21       Trump_2020      thank     0.429942\n",
       "22   Roosevelt_1937  democracy     0.425793\n",
       "23     Johnson_1968    billion     0.425429\n",
       "24  Eisenhower_1956    program     0.417399\n",
       "25     Madison_1814      enemy     0.414576\n",
       "26      Monroe_1818      spain     0.412596\n",
       "27      Hoover_1931      banks     0.409236\n",
       "28      Carter_1978         ve     0.407975\n",
       "29       Buren_1839      banks     0.406933\n",
       "30        Bush_2003    hussein     0.406446\n",
       "31        Bush_2003     saddam     0.406446\n",
       "32        Ford_1975        oil     0.403722\n",
       "33       Biden_2021       jobs     0.402133\n",
       "34   Roosevelt_1903    isthmus     0.397411\n",
       "35   Cleveland_1888       1888     0.397201\n",
       "36       Trump_2018    tonight     0.397188\n",
       "37       Hayes_1877    coinage     0.392970\n",
       "38        Bush_2007       iraq     0.392226\n",
       "39        Bush_2008       iraq     0.389564\n",
       "40   Cleveland_1887     tariff     0.387655\n",
       "41       Tyler_1844      texas     0.384964\n",
       "42       Obama_2015         ve     0.379427\n",
       "43      Truman_1953  communist     0.378838\n",
       "44        Bush_2001     budget     0.378685\n",
       "45       Trump_2019      thank     0.377305\n",
       "46        Bush_1989    tonight     0.372753\n",
       "47      Reagan_1982   programs     0.366303\n",
       "48      Truman_1953     soviet     0.366231\n",
       "49   Cleveland_1893       1893     0.365565"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top 15 tfidf scores for each text\n",
    "N = 15\n",
    "tfidf_long = tfidf_long.sort_values(by = \"tfidf_score\", ascending=False)\n",
    "print(tfidf_long.shape)\n",
    "tfidf_sub = tfidf_long.groupby('textname').head(N).reset_index(drop=True)\n",
    "\n",
    "tfidf_sub.head(50)\n",
    "\n",
    "#textnames = list(tfidf_long['textname'].unique())\n",
    "\n",
    "#for i, text in enumerate(textnames):\n",
    "#    onetext_df = tfidf_sub[tfidf_sub['textname'] == text]\n",
    "#    print(onetext_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEMMAS?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\F0040RP\\Documents\\DartLib_RDS\\intro-to-python\\TAW\\2b_completed_tfidf.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m lemma_tokenizer \u001b[39m=\u001b[39m LemmaTokenizer()                                 \u001b[39m###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m eng_stops \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))                        \u001b[39m###\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m lemma_stop \u001b[39m=\u001b[39m lemma_tokenizer(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(eng_stops))                  \u001b[39m###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtfidf_analysis\u001b[39m(textdir, ng_range \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), lemmas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m    textdir = pathlib Path object to folder containing .txt files to be analyzed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m    ng_range = range of ngrams to be analyzed, i.e. (1,2) will analyze words of length 1 (unigrams) and 2 (bigrams) \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m    1. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\F0040RP\\Documents\\DartLib_RDS\\intro-to-python\\TAW\\2b_completed_tfidf.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc):                                        \u001b[39m###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/F0040RP/Documents/DartLib_RDS/intro-to-python/TAW/2b_completed_tfidf.ipynb#X63sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwnl\u001b[39m.\u001b[39mlemmatize(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(doc) \u001b[39mif\u001b[39;00m t \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_tokens]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer   ###\n",
    "from nltk.corpus import stopwords\n",
    "stop = sorted(stopwords.words('english'))\n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:                                               ###\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']      ###\n",
    "    def __init__(self):                                             ###\n",
    "        self.wnl = WordNetLemmatizer()                              ###\n",
    "    def __call__(self, doc):                                        ###\n",
    "        #return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "        return [self.wnl.lemmatize(t) for t in tokenizer.tokenize(doc) if t not in self.ignore_tokens]    ###\n",
    "    \n",
    "lemma_tokenizer = LemmaTokenizer()                                 ###\n",
    "eng_stops = set(stopwords.words('english'))                        ###\n",
    "lemma_stop = lemma_tokenizer(' '.join(eng_stops))                  ###\n",
    "\n",
    "def tfidf_analysis(textdir, ng_range = (1,1), lemmas = False):\n",
    "    '''\n",
    "    textdir = pathlib Path object to folder containing .txt files to be analyzed\n",
    "    ng_range = range of ngrams to be analyzed, i.e. (1,2) will analyze words of length 1 (unigrams) and 2 (bigrams) \n",
    "    reads in a file folder and returns a long tfidf dataframe for all .txt files found in this folder\n",
    "    Steps:\n",
    "    1. \n",
    "    '''\n",
    "    #tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', ngram_range = (ng_range))\n",
    "    if lemmas:\n",
    "        tfidf_vectorizer = TfidfVectorizer(input = \"filename\", stop_words = lemma_stop, tokenizer = lemma_tokenizer, ngram_range = (ng_range), max_df = 0.5, max_features=5000)  #$$$$\n",
    "    else:\n",
    "        tfidf_vectorizer = TfidfVectorizer(input = \"filename\", stop_words = \"english\", ngram_range = (ng_range), max_df = 0.5, max_features=5000)  #$$$$\n",
    "        \n",
    "    pathlist = sorted(textdir.glob('*.txt'))\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(pathlist)\n",
    "    text_titles = [path.stem for path in pathlist]\n",
    "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    print(\"df shape: \", tfidf_df.shape)\n",
    "    tfidf_df = tfidf_df.loc[: ,(tfidf_df.max(numeric_only = True) > 0.02)]\n",
    "    print(\"df shape: \", tfidf_df.shape)\n",
    "    #print(tfidf_df.head())\n",
    "    tfidf_df.index.name = \"textname\"\n",
    "    tfidf_df = tfidf_df.reset_index()\n",
    "    tfidf_long =  pd.melt(tfidf_df, id_vars = \"textname\", var_name = \"word\", value_name = \"tfidf_score\", value_vars = list(tfidf_df.drop(columns = [\"textname\"]).columns))\n",
    "    \n",
    "    tfidf_long = tfidf_long.sort_values(by = 'tfidf_score', ascending = False)\n",
    "    print(\"df shape: \", tfidf_long.shape)\n",
    "    print(tfidf_long.head(10))\n",
    "    return(tfidf_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\F0040RP\\Documents\\DartLib_RDS\\intro-to-python\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning:\n",
      "\n",
      "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (233, 5000)\n",
      "df shape:  (233, 5000)\n",
      "df shape:  (1165000, 3)\n",
      "                textname       word  tfidf_score\n",
      "431            Taft_1910         00     0.799647\n",
      "1127606     Johnson_1966    vietnam     0.668202\n",
      "995180       Carter_1980     soviet     0.626473\n",
      "243          Arthur_1883         00     0.624692\n",
      "242          Arthur_1882         00     0.590526\n",
      "533848    Cleveland_1895       gold     0.579545\n",
      "214643      Clinton_1996  challenge     0.548781\n",
      "706157         Polk_1846     mexico     0.543914\n",
      "706158         Polk_1847     mexico     0.538077\n",
      "29426    Eisenhower_1961       1953     0.528814\n",
      "(3495, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taft_1910</td>\n",
       "      <td>00</td>\n",
       "      <td>0.799647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson_1966</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.668202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carter_1980</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.626473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arthur_1883</td>\n",
       "      <td>00</td>\n",
       "      <td>0.624692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arthur_1882</td>\n",
       "      <td>00</td>\n",
       "      <td>0.590526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>gold</td>\n",
       "      <td>0.579545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clinton_1996</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.548781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.543914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polk_1847</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.538077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eisenhower_1961</td>\n",
       "      <td>1953</td>\n",
       "      <td>0.528814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Truman_1953</td>\n",
       "      <td>communist</td>\n",
       "      <td>0.522348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Harrison_1892</td>\n",
       "      <td>1892</td>\n",
       "      <td>0.513597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Reagan_1982</td>\n",
       "      <td>program</td>\n",
       "      <td>0.508172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Obama_2013</td>\n",
       "      <td>job</td>\n",
       "      <td>0.498189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Roosevelt_1937</td>\n",
       "      <td>democracy</td>\n",
       "      <td>0.491312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Eisenhower_1956</td>\n",
       "      <td>program</td>\n",
       "      <td>0.486587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Truman_1946</td>\n",
       "      <td>1947</td>\n",
       "      <td>0.485501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>gentleman</td>\n",
       "      <td>0.469151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Monroe_1819</td>\n",
       "      <td>spain</td>\n",
       "      <td>0.467331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Roosevelt_1943</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.449489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Adams_1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>0.448437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Eisenhower_1955</td>\n",
       "      <td>program</td>\n",
       "      <td>0.446993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Johnson_1967</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.441542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Obama_2012</td>\n",
       "      <td>job</td>\n",
       "      <td>0.440915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Truman_1951</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.440022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Arthur_1881</td>\n",
       "      <td>00</td>\n",
       "      <td>0.437827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hoover_1930</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.436741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>texas</td>\n",
       "      <td>0.434742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Biden_2021</td>\n",
       "      <td>job</td>\n",
       "      <td>0.432597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bush_2002</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>0.427308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Johnson_1968</td>\n",
       "      <td>billion</td>\n",
       "      <td>0.419485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Trump_2020</td>\n",
       "      <td>thank</td>\n",
       "      <td>0.415482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Monroe_1818</td>\n",
       "      <td>spain</td>\n",
       "      <td>0.410798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>hussein</td>\n",
       "      <td>0.409852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>saddam</td>\n",
       "      <td>0.409852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Eisenhower_1954</td>\n",
       "      <td>program</td>\n",
       "      <td>0.409821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Obama_2014</td>\n",
       "      <td>job</td>\n",
       "      <td>0.399991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Bush_2001</td>\n",
       "      <td>budget</td>\n",
       "      <td>0.399576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Cleveland_1888</td>\n",
       "      <td>1888</td>\n",
       "      <td>0.399022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Nixon_1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>0.397598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Hayes_1877</td>\n",
       "      <td>coinage</td>\n",
       "      <td>0.396098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Trump_2018</td>\n",
       "      <td>tonight</td>\n",
       "      <td>0.391911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Bush_1989</td>\n",
       "      <td>tonight</td>\n",
       "      <td>0.390374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Roosevelt_1903</td>\n",
       "      <td>isthmus</td>\n",
       "      <td>0.389295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Tyler_1844</td>\n",
       "      <td>texas</td>\n",
       "      <td>0.388885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Carter_1979</td>\n",
       "      <td>salt</td>\n",
       "      <td>0.383243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Cleveland_1887</td>\n",
       "      <td>tariff</td>\n",
       "      <td>0.379538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Bush_2007</td>\n",
       "      <td>iraq</td>\n",
       "      <td>0.378413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Trump_2019</td>\n",
       "      <td>thank</td>\n",
       "      <td>0.374803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Madison_1814</td>\n",
       "      <td>enemy</td>\n",
       "      <td>0.368921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textname       word  tfidf_score\n",
       "0         Taft_1910         00     0.799647\n",
       "1      Johnson_1966    vietnam     0.668202\n",
       "2       Carter_1980     soviet     0.626473\n",
       "3       Arthur_1883         00     0.624692\n",
       "4       Arthur_1882         00     0.590526\n",
       "5    Cleveland_1895       gold     0.579545\n",
       "6      Clinton_1996  challenge     0.548781\n",
       "7         Polk_1846     mexico     0.543914\n",
       "8         Polk_1847     mexico     0.538077\n",
       "9   Eisenhower_1961       1953     0.528814\n",
       "10      Truman_1953  communist     0.522348\n",
       "11    Harrison_1892       1892     0.513597\n",
       "12      Reagan_1982    program     0.508172\n",
       "13       Obama_2013        job     0.498189\n",
       "14   Roosevelt_1937  democracy     0.491312\n",
       "15  Eisenhower_1956    program     0.486587\n",
       "16      Truman_1946       1947     0.485501\n",
       "17       Adams_1800  gentleman     0.469151\n",
       "18      Monroe_1819      spain     0.467331\n",
       "19   Roosevelt_1943       1942     0.449489\n",
       "20       Adams_1827       1827     0.448437\n",
       "21  Eisenhower_1955    program     0.446993\n",
       "22     Johnson_1967    vietnam     0.441542\n",
       "23       Obama_2012        job     0.440915\n",
       "24      Truman_1951     soviet     0.440022\n",
       "25      Arthur_1881         00     0.437827\n",
       "26      Hoover_1930       1928     0.436741\n",
       "27        Polk_1846      texas     0.434742\n",
       "28       Biden_2021        job     0.432597\n",
       "29        Bush_2002  terrorist     0.427308\n",
       "30     Johnson_1968    billion     0.419485\n",
       "31       Trump_2020      thank     0.415482\n",
       "32      Monroe_1818      spain     0.410798\n",
       "33        Bush_2003    hussein     0.409852\n",
       "34        Bush_2003     saddam     0.409852\n",
       "35  Eisenhower_1954    program     0.409821\n",
       "36       Obama_2014        job     0.399991\n",
       "37        Bush_2001     budget     0.399576\n",
       "38   Cleveland_1888       1888     0.399022\n",
       "39       Nixon_1974       1974     0.397598\n",
       "40       Hayes_1877    coinage     0.396098\n",
       "41       Trump_2018    tonight     0.391911\n",
       "42        Bush_1989    tonight     0.390374\n",
       "43   Roosevelt_1903    isthmus     0.389295\n",
       "44       Tyler_1844      texas     0.388885\n",
       "45      Carter_1979       salt     0.383243\n",
       "46   Cleveland_1887     tariff     0.379538\n",
       "47        Bush_2007       iraq     0.378413\n",
       "48       Trump_2019      thank     0.374803\n",
       "49     Madison_1814      enemy     0.368921"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take 4x longer to run with lemmatizing tokenizer!\n",
    "N = 15\n",
    "longdf = tfidf_analysis(textdir, (1,1), lemmas = True)\n",
    "longdf = longdf.sort_values(by = \"tfidf_score\", ascending=False)\n",
    "longdf_sub = longdf.groupby('textname').head(N).reset_index(drop=True)\n",
    "print(longdf_sub.shape)\n",
    "longdf_sub.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. TFIDF vectors with ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\F0040RP\\Documents\\DartLib_RDS\\intro-to-python\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning:\n",
      "\n",
      "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (233, 5000)\n",
      "df shape:  (233, 5000)\n",
      "df shape:  (1165000, 3)\n",
      "               textname            word  tfidf_score\n",
      "901506        Bush_2003  saddam hussein     0.686624\n",
      "933818   Roosevelt_1936       shall say     0.676655\n",
      "810740     Madison_1813    prisoner war     0.580559\n",
      "383727      Truman_1953      free world     0.578242\n",
      "458592     Clinton_1994     health care     0.568968\n",
      "327         Hoover_1930         000 000     0.539927\n",
      "938359  Eisenhower_1961      since 1953     0.538360\n",
      "382327      Truman_1951     free nation     0.536234\n",
      "383585  Eisenhower_1960      free world     0.535518\n",
      "656861        Bush_2008      must trust     0.531432\n",
      "(3495, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>saddam hussein</td>\n",
       "      <td>0.686624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roosevelt_1936</td>\n",
       "      <td>shall say</td>\n",
       "      <td>0.676655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madison_1813</td>\n",
       "      <td>prisoner war</td>\n",
       "      <td>0.580559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truman_1953</td>\n",
       "      <td>free world</td>\n",
       "      <td>0.578242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinton_1994</td>\n",
       "      <td>health care</td>\n",
       "      <td>0.568968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hoover_1930</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.539927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eisenhower_1961</td>\n",
       "      <td>since 1953</td>\n",
       "      <td>0.538360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Truman_1951</td>\n",
       "      <td>free nation</td>\n",
       "      <td>0.536234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Eisenhower_1960</td>\n",
       "      <td>free world</td>\n",
       "      <td>0.535518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bush_2008</td>\n",
       "      <td>must trust</td>\n",
       "      <td>0.531432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hoover_1929</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.518962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Truman_1946</td>\n",
       "      <td>year 1947</td>\n",
       "      <td>0.513969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Roosevelt_1938</td>\n",
       "      <td>purchasing power</td>\n",
       "      <td>0.508378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Johnson_1966</td>\n",
       "      <td>south vietnam</td>\n",
       "      <td>0.507762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Carter_1980</td>\n",
       "      <td>soviet union</td>\n",
       "      <td>0.506790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Clinton_1995</td>\n",
       "      <td>new covenant</td>\n",
       "      <td>0.503845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Clinton_1999</td>\n",
       "      <td>21st century</td>\n",
       "      <td>0.503479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Biden_2021</td>\n",
       "      <td>job plan</td>\n",
       "      <td>0.499435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Roosevelt_1944</td>\n",
       "      <td>national service</td>\n",
       "      <td>0.498830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Clinton_1996</td>\n",
       "      <td>challenge congress</td>\n",
       "      <td>0.496977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Truman_1947</td>\n",
       "      <td>labor management</td>\n",
       "      <td>0.479361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Coolidge_1925</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.479176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Grant_1870</td>\n",
       "      <td>san domingo</td>\n",
       "      <td>0.475740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Hoover_1931</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.464451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bush_2005</td>\n",
       "      <td>social security</td>\n",
       "      <td>0.461512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Obama_2009</td>\n",
       "      <td>health care</td>\n",
       "      <td>0.452643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nixon_1971</td>\n",
       "      <td>great goal</td>\n",
       "      <td>0.443505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Trump_2020</td>\n",
       "      <td>thank much</td>\n",
       "      <td>0.443324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Reagan_1988</td>\n",
       "      <td>7 year</td>\n",
       "      <td>0.441277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Roosevelt_1943</td>\n",
       "      <td>time greater</td>\n",
       "      <td>0.436582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Coolidge_1928</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.429157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Truman_1946</td>\n",
       "      <td>million dollar</td>\n",
       "      <td>0.428508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Roosevelt_1942</td>\n",
       "      <td>united nation</td>\n",
       "      <td>0.426568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Bush_2007</td>\n",
       "      <td>al qaeda</td>\n",
       "      <td>0.423494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>state note</td>\n",
       "      <td>0.413647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Coolidge_1924</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.411829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Johnson_1865</td>\n",
       "      <td>general government</td>\n",
       "      <td>0.411118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Monroe_1819</td>\n",
       "      <td>catholic majesty</td>\n",
       "      <td>0.408272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Coolidge_1926</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.407896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Adams_1826</td>\n",
       "      <td>000 000</td>\n",
       "      <td>0.407397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Harrison_1892</td>\n",
       "      <td>per cent</td>\n",
       "      <td>0.403682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Truman_1950</td>\n",
       "      <td>50 year</td>\n",
       "      <td>0.403466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Bush_2008</td>\n",
       "      <td>al qaeda</td>\n",
       "      <td>0.402532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Bush_2001</td>\n",
       "      <td>social security</td>\n",
       "      <td>0.393826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Roosevelt_1939</td>\n",
       "      <td>billion dollar</td>\n",
       "      <td>0.393225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Clinton_1993</td>\n",
       "      <td>health care</td>\n",
       "      <td>0.391926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Wilson_1916</td>\n",
       "      <td>commerce commission</td>\n",
       "      <td>0.387464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wilson_1917</td>\n",
       "      <td>german people</td>\n",
       "      <td>0.387259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Roosevelt_1943</td>\n",
       "      <td>united nation</td>\n",
       "      <td>0.386123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Clinton_1998</td>\n",
       "      <td>21st century</td>\n",
       "      <td>0.385521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textname                 word  tfidf_score\n",
       "0         Bush_2003       saddam hussein     0.686624\n",
       "1    Roosevelt_1936            shall say     0.676655\n",
       "2      Madison_1813         prisoner war     0.580559\n",
       "3       Truman_1953           free world     0.578242\n",
       "4      Clinton_1994          health care     0.568968\n",
       "5       Hoover_1930              000 000     0.539927\n",
       "6   Eisenhower_1961           since 1953     0.538360\n",
       "7       Truman_1951          free nation     0.536234\n",
       "8   Eisenhower_1960           free world     0.535518\n",
       "9         Bush_2008           must trust     0.531432\n",
       "10      Hoover_1929              000 000     0.518962\n",
       "11      Truman_1946            year 1947     0.513969\n",
       "12   Roosevelt_1938     purchasing power     0.508378\n",
       "13     Johnson_1966        south vietnam     0.507762\n",
       "14      Carter_1980         soviet union     0.506790\n",
       "15     Clinton_1995         new covenant     0.503845\n",
       "16     Clinton_1999         21st century     0.503479\n",
       "17       Biden_2021             job plan     0.499435\n",
       "18   Roosevelt_1944     national service     0.498830\n",
       "19     Clinton_1996   challenge congress     0.496977\n",
       "20      Truman_1947     labor management     0.479361\n",
       "21    Coolidge_1925              000 000     0.479176\n",
       "22       Grant_1870          san domingo     0.475740\n",
       "23      Hoover_1931              000 000     0.464451\n",
       "24        Bush_2005      social security     0.461512\n",
       "25       Obama_2009          health care     0.452643\n",
       "26       Nixon_1971           great goal     0.443505\n",
       "27       Trump_2020           thank much     0.443324\n",
       "28      Reagan_1988               7 year     0.441277\n",
       "29   Roosevelt_1943         time greater     0.436582\n",
       "30    Coolidge_1928              000 000     0.429157\n",
       "31      Truman_1946       million dollar     0.428508\n",
       "32   Roosevelt_1942        united nation     0.426568\n",
       "33        Bush_2007             al qaeda     0.423494\n",
       "34   Cleveland_1895           state note     0.413647\n",
       "35    Coolidge_1924              000 000     0.411829\n",
       "36     Johnson_1865   general government     0.411118\n",
       "37      Monroe_1819     catholic majesty     0.408272\n",
       "38    Coolidge_1926              000 000     0.407896\n",
       "39       Adams_1826              000 000     0.407397\n",
       "40    Harrison_1892             per cent     0.403682\n",
       "41      Truman_1950              50 year     0.403466\n",
       "42        Bush_2008             al qaeda     0.402532\n",
       "43        Bush_2001      social security     0.393826\n",
       "44   Roosevelt_1939       billion dollar     0.393225\n",
       "45     Clinton_1993          health care     0.391926\n",
       "46      Wilson_1916  commerce commission     0.387464\n",
       "47      Wilson_1917        german people     0.387259\n",
       "48   Roosevelt_1943        united nation     0.386123\n",
       "49     Clinton_1998         21st century     0.385521"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 15\n",
    "ng_longdf = tfidf_analysis(textdir, (2,2), lemmas = True)\n",
    "ng_longdf = ng_longdf.sort_values(by = \"tfidf_score\", ascending=False)\n",
    "ng_longdf_sub = ng_longdf.groupby('textname').head(N).reset_index(drop=True)\n",
    "print(ng_longdf_sub.shape)\n",
    "ng_longdf_sub.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_longdf_sub.to_csv(\"sotu_2grams_tfidf_5000maxfeats_top15.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\F0040RP\\Documents\\DartLib_RDS\\intro-to-python\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning:\n",
      "\n",
      "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.43991929 0.403336   ... 0.19580329 0.18803809 0.20870311]\n",
      " [0.43991929 1.         0.36556119 ... 0.18971342 0.18305033 0.18292755]\n",
      " [0.403336   0.36556119 1.         ... 0.20212655 0.19737592 0.20642286]\n",
      " ...\n",
      " [0.19580329 0.18971342 0.20212655 ... 1.         0.40592595 0.31038614]\n",
      " [0.18803809 0.18305033 0.19737592 ... 0.40592595 1.         0.38413319]\n",
      " [0.20870311 0.18292755 0.20642286 ... 0.31038614 0.38413319 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tfidf_vectorizer3 = TfidfVectorizer(input = \"filename\", stop_words = lemma_stop, tokenizer = lemma_tokenizer)\n",
    "tfidf_matrix = tfidf_vectorizer3.fit_transform(pathlist)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 233)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
