{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People & Places: Named Entity Recognition\n",
    "\n",
    "A Jupyter Notebook created for a **Reproducible Research Workshop**\n",
    "\n",
    "(A Collaboration between Dartmouth Library and Research Computing)\n",
    "\n",
    "[*Click here to view or register for our current list of workshops*](http://dartgo.org/RRADworkshops), including a workshop next week on [**Stylometry**](https://libcal.dartmouth.edu/event/11237111) (the study of a text's or author's style - to discover authorship of anonymous documents, among other things) and other workshops on Bibliometric Analysis, the customized use of Large Language Models, Text Generation in R, and more....\n",
    "\n",
    "*Created by*:\n",
    "+ Jeremy Mikecz, Research Data Services (Dartmouth Library)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will learn how to extract named entities (names of people, places, groups, institutions, etc.) from text files and then analyze the results.\n",
    "\n",
    "For example, one of our goals is to create a map of all countries mentioned in the State of the Union corpus. \n",
    "\n",
    "What steps do you anticipate we will have to do in order to successfully accomplish this project? List the steps in the markdown below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Using spaCy\n",
    "\n",
    "**spaCy** is a Python package / library designed to enable fast and efficient Natural Language Processing (for more: [What's spaCy?](https://spacy.io/usage/spacy-101#whats-spacy)). In a previous lesson we used the **Natural Language ToolKit** (NLTK), which does similar things. However, where NLTK is largely designed for instruction and research, spaCy is designed for production, including the fast processing of large amounts of text.\n",
    "\n",
    "It offers a variety of Natural Language Processing (NLP) features, including:\n",
    "+ pre-processing tools\n",
    "    + tokenization - dividing text into words (and punctuation marks, numbers, etc.)\n",
    "    + sentence boundary detection - dividing texts into sentences\n",
    "    + lemmatization - identifying the root or base form of words\n",
    "+ Linguistic Annotations\n",
    "    + Part-of-speech tags (POS) and dependencies - tags part of speech (noun, proper noun, verb, adjective, etc.) and dependency (which words modify which other words? adjectives --> nouns; subject --> verb --> object, etc.)\n",
    "    + Named Entity Recognition (NER) - identifying names of objects, whether people, places, organizations, or other \"entities\" like book or product titles\n",
    "    + word vectorization - word vectors assign numerical values to words placing each into a multi-dimensional space where similar words are found in close proximity to one another\n",
    "\n",
    "And much more....\n",
    "\n",
    "### Working with Foreign Languages\n",
    "\n",
    "+ You can use [spaCy's existing language models](https://spacy.io/usage/models) for languages from English, Spanish, and Mandarin Chinese to Kyrgyz and Yoruba.\n",
    "+ you can modify one of these existing language models\n",
    "+ or you can create a new language model from scratch\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Installing spaCy\n",
    "\n",
    "If you were to want to run spaCy on your own computer, [the spaCy instructions](https://spacy.io/usage) recommend installing spaCy in a *virtual environment*. After activating a virtual environment, you would run the following in a terminal:\n",
    "\n",
    "```\n",
    "python -m venv .env    #to activate already established virtual environment called \".venv\"\n",
    "source .env/bin/activate     #to activate .venv\n",
    "pip install -U pip setuptools wheel   \n",
    "pip install -U spacy             #installs spaCy\n",
    "python -m spacy download en_core_web_sm   #installs English model from spaCy\n",
    "```\n",
    "\n",
    "However, **we are going to install spaCy in JupyterHub**. To do so, we will need to uncomment (remove the \"#\") the following cells and run on JupyterHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. About Named Entity Recognition with spaCy\n",
    "\n",
    "Basic named entity recognizers commonly identify the following types of entities:\n",
    "\n",
    "```\n",
    "place names\n",
    "person names\n",
    "group names\n",
    "miscellaneous / other entities\n",
    "```\n",
    "\n",
    "**spaCy**'s NER identifies a wider-range of entities.\n",
    "\n",
    "Examine the list of [entity types identified by spaCy](https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218) below:\n",
    "\n",
    "```\n",
    "PERSON:      People, including fictional.\n",
    "NORP:        Nationalities or religious or political groups.\n",
    "FAC:         Buildings, airports, highways, bridges, etc.\n",
    "ORG:         Companies, agencies, institutions, etc.\n",
    "GPE:         Countries, cities, states.\n",
    "LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART: Titles of books, songs, etc.\n",
    "LAW:         Named documents made into laws.\n",
    "LANGUAGE:    Any named language.\n",
    "DATE:        Absolute or relative dates or periods.\n",
    "TIME:        Times smaller than a day.\n",
    "PERCENT:     Percentage, including ”%“.\n",
    "MONEY:       Monetary values, including unit.\n",
    "QUANTITY:    Measurements, as of weight or distance.\n",
    "ORDINAL:     “first”, “second”, etc.\n",
    "CARDINAL:    Numerals that do not fall under another type.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainstorm some ways you could use Named Entity Recognition in your research field for the types of texts and documents researchers in that field typically deal with. What projects can you envision? What questions could you answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Import Packages\n",
    "\n",
    "First, let's import all necessary Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "import pandas as pd\n",
    "from spacy.lang.en.examples import sentences\n",
    "from spacy import displacy   #for visualizing word types and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, since we will be working with English texts, we need to import one of spaCy's English models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spacy.io/models/en\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Linguistic Tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple is looking at buying U.K. startup for $1 billion', 'Autonomous cars shift insurance liability toward manufacturers', 'San Francisco considers banning sidewalk delivery robots', 'London is a big city in the United Kingdom.', 'Where are you?', 'Who is the president of France?', 'What is the capital of the United States?', 'When was Barack Obama born?']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion\n",
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup NOUN dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentences[0])                  #try substituting sentence #0 with another sentence\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">Apply spaCy's Part-of-Speech (POS) detection to a sentence or short text of your choice. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dep xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Applying Named Entity Recognition (NER) in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move beyond spaCy's sample sentences and try extracting named entities (NEs) from song lyrics and other texts. Examine the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London 1 7 GPE\n",
      "Paris 9 14 GPE\n",
      "Monte Carlo 16 27 PERSON\n",
      "Germany 29 36 GPE\n",
      "Rome 41 45 GPE\n",
      "China 105 110 GPE\n",
      "Holland 112 119 GPE\n",
      "Belgium 121 128 GPE\n",
      "Rio 130 133 GPE\n",
      "Africa 135 141 LOC\n",
      "Japan 143 148 GPE\n",
      "Feelin 243 249 PRODUCT\n",
      "Boyz II Men 321 332 ORG\n",
      "Keisha 465 471 PERSON\n",
      "Kelly 473 478 PERSON\n",
      "Tonya 480 485 GPE\n",
      "Stacy 487 492 ORG\n",
      "Mica 494 498 PERSON\n",
      "LaShaun 503 510 GPE\n",
      "Trina 518 523 GPE\n",
      "Carla 525 530 PERSON\n",
      "Lisa 532 536 PERSON\n",
      "Cheri 538 543 PERSON\n",
      "Diane 549 554 PERSON\n",
      "Feelin 710 716 PRODUCT\n",
      "Boyz II Men 788 799 ORG\n",
      "Houston 939 946 GPE\n",
      "Phoenix 948 955 GPE\n",
      "Carolina 957 965 GPE\n",
      "Jersey 967 973 GPE\n",
      "the Keys\n",
      "Denver 979 994 FAC\n",
      "Boston 996 1002 GPE\n",
      "Mississippi 1004 1015 GPE\n",
      "Georgia 1017 1024 GPE\n",
      "Tennessee 1026 1035 GPE\n",
      "Dallas 1036 1042 GPE\n",
      "Cleveland 1044 1053 GPE\n",
      "Cali 1055 1059 GPE\n",
      "Philly 1061 1067 GPE\n",
      "New York 1069 1077 GPE\n",
      "DC 1083 1085 GPE\n"
     ]
    }
   ],
   "source": [
    "#below: partial lyrics from Boyz II Men's \"All Around the World\"\n",
    "lyrics = \"\"\"\n",
    "London, Paris, Monte Carlo, Germany and Rome\n",
    "Different places, different faces still it feels like home\n",
    "China, Holland, Belgium, Rio, Africa, Japan\n",
    "That's the way we live and we do the best we can\n",
    "Here we go on another tour on the road again\n",
    "Feelin' good it's alright\n",
    "Just enjoying ourselves\n",
    "Come and take a flight with\n",
    "Boyz II Men back around the world\n",
    "And we're comin' through your town\n",
    "All we do is for you\n",
    "'Cause you've always been there\n",
    "And we appreciate you\n",
    "Keisha, Kelly, Tonya, Stacy, Mica and LaShaun\n",
    "Kathy, Trina, Carla, Lisa, Cheri, and Diane\n",
    "All these girls around the world are fly in every land\n",
    "And it's hard to choose, but there's one for every man\n",
    "Here we go on another tour on the road again\n",
    "Feelin' good it's alright\n",
    "Just enjoying ourselves\n",
    "Come and take a flight with\n",
    "Boyz II Men back around the world\n",
    "And we're comin' through your town\n",
    "All we do, we do it for you\n",
    "'Cause you've always been there\n",
    "And we appreciate you\n",
    "Houston, Phoenix, Carolina, Jersey, and the Keys\n",
    "Denver, Boston, Mississippi, Georgia, Tennessee\n",
    "Dallas, Cleveland, Cali, Philly, New York, and DC\n",
    "That's the life we live and it's the only life\n",
    "\"\"\"\n",
    "\n",
    "doc_lyrics = nlp(lyrics)                  \n",
    "for ent in doc_lyrics.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand these results, we need to understand what this model was trained on. At the beginning of this notebook we imported the **en_core_web_sm** model. Let's examine [spaCy's documentation for this model](https://spacy.io/models/en). \n",
    "\n",
    "What texts / sources was this model trained on?\n",
    "\n",
    "How does it differ from spaCy's other English models?\n",
    "\n",
    "How accurate are these models at NER?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can experiment with different types of text. Below, we will extract NEs from an excerpt from Jack Kerouac's *On the Road* (1957)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one 6 9 CARDINAL\n",
      "1 10 11 CARDINAL\n",
      "first 15 20 ORDINAL\n",
      "Dean 25 29 PERSON\n",
      "Dean Moriarty 274 287 ORG\n",
      "Dean 460 464 PERSON\n",
      "Salt Lake City 577 591 GPE\n",
      "1926 595 599 DATE\n",
      "Los Angeles 630 641 GPE\n",
      "First 643 648 ORDINAL\n",
      "Chad King 683 692 PERSON\n",
      "New Mexico 745 755 GPE\n",
      "Chad 858 862 GPE\n",
      "Nietzsche 886 895 PERSON\n",
      "Chad 943 947 GPE\n",
      "one 957 960 CARDINAL\n",
      "Carlo 967 972 PERSON\n",
      "Dean Moriarty 1051 1064 ORG\n",
      "Dean 1093 1097 PERSON\n",
      "today 1120 1125 DATE\n",
      "Dean 1196 1200 PERSON\n",
      "New York 1244 1252 GPE\n",
      "first 1261 1266 ORDINAL\n",
      "Marylou 1332 1339 PRODUCT\n",
      "One day 1341 1348 DATE\n",
      "Chad 1385 1389 GPE\n",
      "Tim Gray 1394 1402 PERSON\n",
      "Dean 1411 1415 PERSON\n",
      "East Harlem 1451 1462 LOC\n",
      "the Spanish Harlem 1464 1482 ORG\n",
      "Dean 1484 1488 PERSON\n",
      "the night before 1501 1517 TIME\n",
      "first 1523 1528 ORDINAL\n",
      "New York 1537 1545 GPE\n",
      "Greyhound 1611 1620 NORP\n",
      "50th Street 1628 1639 FAC\n",
      "New York 1787 1795 GPE\n",
      "Dean 1800 1804 PERSON\n"
     ]
    }
   ],
   "source": [
    "novel_excerpt = \"\"\"\n",
    "part one\n",
    "1\n",
    "\n",
    "I first met Dean not long after my wife and I split up. I had just gotten over a serious illness that I won’t bother to talk about, except that it had something to do with the miserably weary split-up and my feeling that everything was dead. With the coming of Dean Moriarty began the part of my life you could call my life on the road. Before that I’d often dreamed of going West to see the country, always vaguely planning and never taking off. Dean is the perfect guy for the road because he actually was born on the road, when his parents were passing through Salt Lake City in 1926, in a jalopy, on their way to Los Angeles. First reports of him came to me through Chad King, who’d shown me a few letters from him written in a New Mexico reform school. I was tremendously interested in the letters because they so naively and sweetly asked Chad to teach him all about Nietzsche and all the wonderful intellectual things that Chad knew. At one point Carlo and I talked about the letters and wondered if we would ever meet the strange Dean Moriarty. This is all far back, when Dean was not the way he is today, when he was a young jailkid shrouded in mystery. Then news came that Dean was out of reform school and was coming to New York for the first time; also there was talk that he had just married a girl called Marylou.\n",
    "One day I was hanging around the campus and Chad and Tim Gray told me Dean was staying in a cold-water pad in East Harlem, the Spanish Harlem. Dean had arrived the night before, the first time in New York, with his beautiful little sharp chick Marylou; they got off the Greyhound bus at 50th Street and cut around the corner looking for a place to eat and went right in Hector’s, and since then Hector’s cafeteria has always been a big symbol of New York for Dean. They spent money on beautiful big glazed cakes and creampuffs.\n",
    "\"\"\"\n",
    "doc_kerouac = nlp(novel_excerpt)                  \n",
    "for ent in doc_kerouac.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">Try applying <b>spaCy's named entity recognition (NER)</b> to a text of your choosing. Copy and paste the above code into the cell below, but insert song lyrics, an excerpt from a novel, or other text.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Visualize spaCy tagging using displacy\n",
    "\n",
    "We can visualize NEs, relationships between words (aka. \"dependencies\"), and other linguistic entities using spaCy's visualization tool [displacy](https://spacy.io/universe/project/displacy).\n",
    "For more on how to use displacy see: https://spacy.io/usage/visualizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d48fce089d3d4c9580df71730bb5828b-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-5\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d48fce089d3d4c9580df71730bb5828b-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d48fce089d3d4c9580df71730bb5828b-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = \"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">Use the displacy functions to visualize named entities and dependencies for other texts of your choosing.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Extracting entities from one State of the Union address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we applied basic text analysis methods to a corpus of 233 State of the Union addresses given by Presidents of the United States from 1791 - 2023.\n",
    "\n",
    "In one of those lessons, we stored all 233 State of the Union addresses in one .tsv file (tab separated values). This will save us from having to read in all 233 text files individually. Let's begin by opening this tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pres</th>\n",
       "      <th>year</th>\n",
       "      <th>numtoks</th>\n",
       "      <th>tokens</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>ltoks</th>\n",
       "      <th>ltoks_ns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Obama</td>\n",
       "      <td>2014</td>\n",
       "      <td>7017</td>\n",
       "      <td>['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Obama</td>\n",
       "      <td>2015</td>\n",
       "      <td>6961</td>\n",
       "      <td>['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Obama</td>\n",
       "      <td>2016</td>\n",
       "      <td>5628</td>\n",
       "      <td>['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Trump</td>\n",
       "      <td>2017</td>\n",
       "      <td>5095</td>\n",
       "      <td>['Thank', 'you', 'very', 'much', 'Mr', 'Speake...</td>\n",
       "      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n",
       "      <td>['thank', 'you', 'very', 'much', 'mr', 'speake...</td>\n",
       "      <td>['thank', 'much', 'mr', 'speaker', 'mr', 'vice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Trump</td>\n",
       "      <td>2018</td>\n",
       "      <td>5204</td>\n",
       "      <td>['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "      <td>['mr', 'speaker', 'mr', 'vice', 'president', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pres  year  numtoks                                             tokens  \\\n",
       "156  Obama  2014     7017  ['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...   \n",
       "157  Obama  2015     6961  ['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...   \n",
       "158  Obama  2016     5628  ['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...   \n",
       "207  Trump  2017     5095  ['Thank', 'you', 'very', 'much', 'Mr', 'Speake...   \n",
       "208  Trump  2018     5204  ['Mr', 'Speaker', 'Mr', 'Vice', 'President', '...   \n",
       "\n",
       "                                              fulltext  \\\n",
       "156  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "157  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "158  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "207  Thank you very much. Mr. Speaker, Mr. Vice Pre...   \n",
       "208  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                 ltoks  \\\n",
       "156  ['mr', 'speaker', 'mr', 'vice', 'president', '...   \n",
       "157  ['mr', 'speaker', 'mr', 'vice', 'president', '...   \n",
       "158  ['mr', 'speaker', 'mr', 'vice', 'president', '...   \n",
       "207  ['thank', 'you', 'very', 'much', 'mr', 'speake...   \n",
       "208  ['mr', 'speaker', 'mr', 'vice', 'president', '...   \n",
       "\n",
       "                                              ltoks_ns  \n",
       "156  ['mr', 'speaker', 'mr', 'vice', 'president', '...  \n",
       "157  ['mr', 'speaker', 'mr', 'vice', 'president', '...  \n",
       "158  ['mr', 'speaker', 'mr', 'vice', 'president', '...  \n",
       "207  ['thank', 'much', 'mr', 'speaker', 'mr', 'vice...  \n",
       "208  ['mr', 'speaker', 'mr', 'vice', 'president', '...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sotudir = Path(\"state-of-the-union-dataset\",\"txt\")\n",
    "sotudf = pd.read_csv(\"sotudf.tsv\", encoding = \"utf-8\", sep = \"\\t\", index_col = 0)\n",
    "sotudf = sotudf.sort_values(by = ['year'])\n",
    "sotudf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open one recent SOTU address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Madame Speaker, Mr. Vice President, Members of Congress, and the First Lady of\\nthe United States:\\n\\nI've come here tonight not only to address the distinguished men and women in\\nthis great chamber, but\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the code below, we are opening the 2009 SOTU \n",
    "# presidential address (which would have been Obama's first SOTU address)\n",
    "sotu = sotudf[sotudf['year'] == 2009]['fulltext'].item()\n",
    "sotu[:200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's once again review the list of named entities that spaCy can extract for us:\n",
    "\n",
    "```\n",
    "PERSON:      People, including fictional.\n",
    "NORP:        Nationalities or religious or political groups.\n",
    "FAC:         Buildings, airports, highways, bridges, etc.\n",
    "ORG:         Companies, agencies, institutions, etc.\n",
    "GPE:         Countries, cities, states.\n",
    "LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART: Titles of books, songs, etc.\n",
    "LAW:         Named documents made into laws.\n",
    "LANGUAGE:    Any named language.\n",
    "DATE:        Absolute or relative dates or periods.\n",
    "TIME:        Times smaller than a day.\n",
    "PERCENT:     Percentage, including ”%“.\n",
    "MONEY:       Monetary values, including unit.\n",
    "QUANTITY:    Measurements, as of weight or distance.\n",
    "ORDINAL:     “first”, “second”, etc.\n",
    "CARDINAL:    Numerals that do not fall under another type.\n",
    "```\n",
    "\n",
    "In the markdown cell below, brainstorm some different research questions you could use spaCy's NER to help you answer (given the list of entities it could help you analyze) about a given SOTU address.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin processing this text with spaCy, we will read it into the spaCy **nlp** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sotu = nlp(sotu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve named entities from the nlp object using the **.ents** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congress 47 55 ORG\n",
      "First 65 70 ORDINAL\n",
      "the United States 79 96 GPE\n",
      "tonight 114 121 TIME\n",
      "Americans 292 301 NORP\n",
      "tonight 1136 1143 TIME\n",
      "American 1157 1165 NORP\n",
      "the United States of America 1219 1247 GPE\n",
      "Earth 1584 1589 LOC\n",
      "America 1623 1630 GPE\n"
     ]
    }
   ],
   "source": [
    "for i, ent in enumerate(doc_sotu.ents):\n",
    "    if i < 10:   # here, we are indicating we only want to return the first 10 entities\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what these labels mean, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even print out these explanations with each entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congress 47 55 ORG Companies, agencies, institutions, etc.\n",
      "First 65 70 ORDINAL \"first\", \"second\", etc.\n",
      "the United States 79 96 GPE Countries, cities, states\n",
      "tonight 114 121 TIME Times smaller than a day\n",
      "Americans 292 301 NORP Nationalities or religious or political groups\n",
      "tonight 1136 1143 TIME Times smaller than a day\n",
      "American 1157 1165 NORP Nationalities or religious or political groups\n",
      "the United States of America 1219 1247 GPE Countries, cities, states\n",
      "Earth 1584 1589 LOC Non-GPE locations, mountain ranges, bodies of water\n",
      "America 1623 1630 GPE Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "for i, ent in enumerate(doc_sotu.ents):\n",
    "    if i < 10:   # here, we are indicating we only want to return the first 10 entities\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the names and labels of these entities into various formats. Below we are saving info about each entity into a tuple and placing those tuples into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Congress', 'ORG', 47, 55), ('First', 'ORDINAL', 65, 70), ('the United States', 'GPE', 79, 96), ('tonight', 'TIME', 114, 121), ('Americans', 'NORP', 292, 301), ('tonight', 'TIME', 1136, 1143), ('American', 'NORP', 1157, 1165), ('the United States of America', 'GPE', 1219, 1247), ('Earth', 'LOC', 1584, 1589), ('America', 'GPE', 1623, 1630)]\n"
     ]
    }
   ],
   "source": [
    "#ents = [(e.text, e.label_, e.kb_id_) for e in doc_sotu.ents]\n",
    "ents = [(e.text, e.label_, e.start_char, e.end_char) for e in doc_sotu.ents]\n",
    "print(ents[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use the start and end character span for each entity to identify it within its context. For example, the code below prints out the entity and the 50 characters immediately preceding and following it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congress = ORG\n",
      "Madame Speaker, Mr. Vice President, Members of Congress, and the First Lady of\n",
      "the United States:\n",
      "\n",
      "I've c\n",
      "First = ORDINAL\n",
      " Mr. Vice President, Members of Congress, and the First Lady of\n",
      "the United States:\n",
      "\n",
      "I've come here tonigh\n",
      "the United States = GPE\n",
      "ident, Members of Congress, and the First Lady of\n",
      "the United States:\n",
      "\n",
      "I've come here tonight not only to address the \n",
      "tonight = TIME\n",
      " First Lady of\n",
      "the United States:\n",
      "\n",
      "I've come here tonight not only to address the distinguished men and wom\n",
      "Americans = NORP\n",
      "and women who\n",
      "sent us here.\n",
      "\n",
      "I know that for many Americans watching right now, the state of our economy is\n",
      "a\n",
      "tonight = TIME\n",
      "are\n",
      "living through difficult and uncertain times, tonight I want every American to\n",
      "know this:\n",
      "\n",
      "We will rebu\n",
      "American = NORP\n",
      "fficult and uncertain times, tonight I want every American to\n",
      "know this:\n",
      "\n",
      "We will rebuild, we will recover, \n",
      "the United States of America = GPE\n",
      "know this:\n",
      "\n",
      "We will rebuild, we will recover, and the United States of America will emerge\n",
      "stronger than before.\n",
      "\n",
      "The weight of \n",
      "Earth = LOC\n",
      "rs and the pride of the hardest-working people\n",
      "on Earth.  Those qualities that have made America the grea\n",
      "America = GPE\n",
      " people\n",
      "on Earth.  Those qualities that have made America the greatest force of\n",
      "progress and prosperity in \n",
      "overnight = TIME\n",
      "he fact is, our economy did not fall into decline overnight.  Nor did all of\n",
      "our problems begin when the hous\n",
      "decades = DATE\n",
      "psed or the stock market sank.\n",
      " We have known for decades that our survival depends on finding new sources \n",
      "today = DATE\n",
      "ng new sources of\n",
      "energy.  Yet we import more oil today than ever before.  The cost of health\n",
      "care eats u\n",
      "each year = DATE\n",
      " health\n",
      "care eats up more and more of our savings each year, yet we keep delaying\n",
      "reform.  Our children will \n",
      "the next quarter = DATE\n",
      " where we failed to look beyond the next\n",
      "payment, the next quarter, or the next election.  A surplus became an excus\n",
      "some other day = DATE\n",
      "ult decisions were put off for some other time on\n",
      "some other day.\n",
      "\n",
      "Well that day of reckoning has arrived, and the\n",
      "that day = DATE\n",
      " off for some other time on\n",
      "some other day.\n",
      "\n",
      "Well that day of reckoning has arrived, and the time to take ch\n",
      "tonight = TIME\n",
      "do, and that's what I'd like to talk to you about tonight.\n",
      "\n",
      "It's an agenda that begins with jobs.\n",
      "\n",
      "As soon \n",
      "Congress = ORG\n",
      "ith jobs.\n",
      "\n",
      "As soon as I took office, I asked this Congress to send me a recovery plan by\n",
      "President's Day tha\n",
      "years = DATE\n",
      "term\n",
      "deficit by assuring weak economic growth for years.  That's why I pushed for\n",
      "quick action.  And toni\n",
      "tonight = TIME\n",
      "ears.  That's why I pushed for\n",
      "quick action.  And tonight, I am grateful that this Congress delivered, and\n",
      "\n",
      "Congress = ORG\n",
      "ick action.  And tonight, I am grateful that this Congress delivered, and\n",
      "pleased to say that the American R\n",
      "the American Recovery and Reinvestment Act = ORG\n",
      " this Congress delivered, and\n",
      "pleased to say that the American Recovery and Reinvestment Act is now law.\n",
      "\n",
      "Over the next two years, this plan w\n",
      "the next two years = DATE\n",
      "n Recovery and Reinvestment Act is now law.\n",
      "\n",
      "Over the next two years, this plan will save or create 3.5 million jobs. \n",
      "3.5 million = CARDINAL\n",
      "the next two years, this plan will save or create 3.5 million jobs.  More\n",
      "than 90% of these jobs will be in the\n",
      "90% = PERCENT\n",
      " will save or create 3.5 million jobs.  More\n",
      "than 90% of these jobs will be in the private sector -- jo\n",
      "57 = CARDINAL\n",
      "nals can continue caring for our sick.\n",
      " There are 57 police officers who are still on the streets of M\n",
      "Minneapolis = GPE\n",
      "7 police officers who are still on the streets of Minneapolis\n",
      "tonight because this plan prevented the layoffs t\n",
      "tonight = TIME\n",
      "icers who are still on the streets of Minneapolis\n",
      "tonight because this plan prevented the layoffs their dep\n",
      "95% = PERCENT\n",
      "artment was about to\n",
      "make.\n",
      "\n",
      "Because of this plan, 95% of the working households in America will receive\n",
      "America = GPE\n",
      "se of this plan, 95% of the working households in America will receive a\n",
      "tax cut -- a tax cut that you will\n",
      "April\n",
      "1st = DATE\n",
      " that you will see in your paychecks beginning on April\n",
      "1st.\n",
      "\n",
      "Because of this plan, families who are struggli\n",
      "2,500 = MONEY\n",
      "e struggling to pay tuition costs will\n",
      "receive a $2,500 tax credit for all four years of college.  And Am\n",
      "all four years = DATE\n",
      "uition costs will\n",
      "receive a $2,500 tax credit for all four years of college.  And Americans who\n",
      "have lost their jo\n",
      "Americans = NORP\n",
      "00 tax credit for all four years of college.  And Americans who\n",
      "have lost their jobs in this recession will b\n",
      "Washington = GPE\n",
      "ill work.  I understand that skepticism.  Here in\n",
      "Washington, we've all seen how quickly good intentions can t\n",
      "Biden = PERSON\n",
      "t right.\n",
      "\n",
      "That is why I have asked Vice President Biden to lead a tough, unprecedented\n",
      "oversight effort -\n",
      "Joe = PERSON\n",
      "ed\n",
      "oversight effort -- because nobody messes with Joe.  I have told each member\n",
      "of my Cabinet as well a\n",
      "Cabinet = ORG\n",
      "y messes with Joe.  I have told each member\n",
      "of my Cabinet as well as mayors and governors across the countr\n",
      "American = NORP\n",
      " that they\n",
      "will be held accountable by me and the American people for every dollar they\n",
      "spend.  I have appoi\n",
      "American = NORP\n",
      "d a new website\n",
      "called recovery.gov so that every American can find out how and where their\n",
      "money is being s\n",
      "first = ORDINAL\n",
      "ing spent.\n",
      "\n",
      "So the recovery plan we passed is the first step in getting our economy back\n",
      "on track.  But i\n",
      "first = ORDINAL\n",
      "ng our economy back\n",
      "on track.  But it is just the first step.  Because even if we manage this plan\n",
      "flawle\n",
      "tonight = TIME\n",
      "nt to speak plainly and candidly about this issue tonight, because every\n",
      "American should know that it direc\n",
      "American = NORP\n",
      " candidly about this issue tonight, because every\n",
      "American should know that it directly affects you and your\n",
      "First = ORDINAL\n",
      "e-start lending.\n",
      "\n",
      "We will do so in several ways.  First, we are creating a new lending fund that\n",
      "represen\n",
      "Second = ORDINAL\n",
      "and entrepreneurs who keep this\n",
      "economy running.\n",
      "\n",
      "Second, we have launched a housing plan that will help r\n",
      "monthly = DATE\n",
      "lies\n",
      "facing the threat of foreclosure lower their monthly payments and re-finance\n",
      "their mortgages.  It's a \n",
      "millions = CARDINAL\n",
      "e he could never hope to afford, but it will\n",
      "help millions of Americans who are struggling with declining ho\n",
      "Americans = NORP\n",
      "ever hope to afford, but it will\n",
      "help millions of Americans who are struggling with declining home values --\n",
      "\n",
      "Americans = NORP\n",
      " who are struggling with declining home values --\n",
      "Americans who will now be able to take advantage of the low\n",
      "today = DATE\n",
      "out.  In fact, the average family\n",
      "who re-finances today can save nearly $2000 per year on their mortgage.\n",
      "nearly $2000 = MONEY\n",
      "the average family\n",
      "who re-finances today can save nearly $2000 per year on their mortgage.\n",
      "\n",
      "Third, we will act w\n",
      "Third = ORDINAL\n",
      "an save nearly $2000 per year on their mortgage.\n",
      "\n",
      "Third, we will act with the full force of the federal g\n",
      "Americans = NORP\n",
      "al government to ensure\n",
      "that the major banks that Americans depend on have enough confidence and\n",
      "enough money\n",
      "any given day = DATE\n",
      "our people and our economy.\n",
      "\n",
      "I understand that on any given day, Wall Street may be more comforted by an\n",
      "approach\n",
      "the day = DATE\n",
      "'t\n",
      "solve the problem.  And our goal is to quicken the day when we re-start\n",
      "lending to the American people a\n",
      "American = NORP\n",
      "o quicken the day when we re-start\n",
      "lending to the American people and American business and end this crisis \n",
      "American = NORP\n",
      "en we re-start\n",
      "lending to the American people and American business and end this crisis once\n",
      "and for all.\n",
      "\n",
      "I\n",
      "American = NORP\n",
      "w taxpayer\n",
      "dollars result in more lending for the American taxpayer.  This time, CEOs\n",
      "won't be able to use t\n",
      "Those days = DATE\n",
      " buy fancy drapes\n",
      "or disappear on a private jet.  Those days are over.\n",
      "\n",
      "Still, this plan will require signific\n",
      "months = DATE\n",
      " result in an economy that sputters along for\n",
      "not months or years, but perhaps a decade.  That would be wo\n",
      "years = DATE\n",
      " an economy that sputters along for\n",
      "not months or years, but perhaps a decade.  That would be worse for o\n",
      "a decade = DATE\n",
      "utters along for\n",
      "not months or years, but perhaps a decade.  That would be worse for our\n",
      "deficit, worse for \n",
      "Congress = ORG\n",
      "tand that when the last administration asked this Congress to provide\n",
      "assistance for struggling banks, Democ\n",
      "Democrats = NORP\n",
      "gress to provide\n",
      "assistance for struggling banks, Democrats and Republicans alike were\n",
      "infuriated by the mism\n",
      "Republicans = NORP\n",
      "de\n",
      "assistance for struggling banks, Democrats and Republicans alike were\n",
      "infuriated by the mismanagement and re\n",
      "American = NORP\n",
      "anagement and results that followed.  So were the\n",
      "American taxpayers.  So was I.\n",
      "\n",
      "So I know how unpopular it\n",
      "a single penny = MONEY\n",
      "with a sense of responsibility.  I\n",
      "will not spend a single penny for the purpose of rewarding a single Wall\n",
      "Street\n",
      "American = NORP\n",
      "siness.  Investors\n",
      "will return to the market, and American families will see their retirement\n",
      "secured once m\n",
      "Congress = ORG\n",
      "urn, and our\n",
      "economy will recover.\n",
      "\n",
      "So I ask this Congress to join me in doing whatever proves necessary.  B\n",
      "Congress = ORG\n",
      "isis of this magnitude never happens again, I ask Congress to move quickly\n",
      "on legislation that will finally \n",
      "America = GPE\n",
      "he short-term.  But the only way to\n",
      "fully restore America's economic strength is to make the long-term inve\n",
      "this century = DATE\n",
      " compete\n",
      "with the rest of the world. The only way this century will be another American\n",
      "century is if we confron\n",
      "American = NORP\n",
      " world. The only way this century will be another American\n",
      "century is if we confront at last the price of ou\n",
      "the next few days = DATE\n",
      "tand to inherit.  That is our responsibility.\n",
      "\n",
      "In the next few days, I will submit a budget to Congress.  So often, w\n",
      "Congress = ORG\n",
      "\n",
      "\n",
      "In the next few days, I will submit a budget to Congress.  So often, we have\n",
      "come to view these documents \n",
      "America = GPE\n",
      "s document differently.  I see it as a vision for America\n",
      "-- as a blueprint for our future.\n",
      "\n",
      "My budget does\n",
      "trillion dollar = MONEY\n",
      "ts the stark reality of what we've inherited -- a trillion dollar\n",
      "deficit, a financial crisis, and a costly recessi\n",
      "Democrats = NORP\n",
      "iven these realities, everyone in this chamber -- Democrats and Republicans\n",
      "-- will have to sacrifice some wo\n",
      "Republicans = NORP\n",
      "lities, everyone in this chamber -- Democrats and Republicans\n",
      "-- will have to sacrifice some worthy priorities \n",
      "one = CARDINAL\n",
      " midst of civil war, we laid railroad tracks from\n",
      "one coast to another that spurred commerce and indust\n",
      "the Industrial Revolution = EVENT\n",
      "urred commerce and industry.  From the turmoil of\n",
      "the Industrial Revolution came a system of public high schools that prepare\n",
      "GI = ORG\n",
      " new age.  In the wake of war and depression, the GI Bill\n",
      "sent a generation to college and created the\n",
      "American = NORP\n",
      "uggle for freedom led to a nation of highways, an American\n",
      "on the moon, and an explosion of technology that \n",
      "the moon = LOC\n",
      "eedom led to a nation of highways, an American\n",
      "on the moon, and an explosion of technology that still shapes\n",
      "thousands = CARDINAL\n",
      "rivate enterprise.  It created the conditions for thousands of entrepreneurs\n",
      "and new businesses to adapt and \n",
      "three = CARDINAL\n",
      "on't need, the budget I submit will invest in the three\n",
      "areas that are absolutely critical to our economi\n",
      "the 21st century = DATE\n",
      "es the power of clean, renewable energy will\n",
      "lead the 21st century.  And yet, it is China that has launched the larg\n",
      "China = GPE\n",
      "nergy will\n",
      "lead the 21st century.  And yet, it is China that has launched the largest\n",
      "effort in history t\n",
      "Germany = GPE\n",
      "echnology, but we've fallen behind countries like Germany and Japan in\n",
      "producing it.  New plug-in hybrids r\n",
      "Japan = GPE\n",
      "ut we've fallen behind countries like Germany and Japan in\n",
      "producing it.  New plug-in hybrids roll off ou\n",
      "Korea = GPE\n",
      "bly lines, but they will\n",
      "run on batteries made in Korea.\n",
      "\n",
      "Well I do not accept a future where the jobs an\n",
      "tomorrow = DATE\n",
      " accept a future where the jobs and industries of tomorrow take\n",
      "root beyond our borders -- and I know you do\n",
      "America = GPE\n",
      "s -- and I know you don't either.  It is time for\n",
      "America to lead again.\n",
      "\n",
      "Thanks to our recovery plan, we w\n",
      "the next three years = DATE\n",
      "ouble this nation's supply of renewable\n",
      "energy in the next three years.  We have also made the largest investment in\n",
      "bas\n",
      "American = NORP\n",
      "e largest investment in\n",
      "basic research funding in American history -- an investment that will spur not\n",
      "only \n",
      "thousands of miles = QUANTITY\n",
      ", science, and\n",
      "technology.\n",
      "\n",
      "We will soon lay down thousands of miles of power lines that can carry new\n",
      "energy to citie\n",
      "Americans = NORP\n",
      "s and towns across this country.  And we will put Americans to\n",
      "work making our homes and buildings more effic\n",
      "billions of dollars = MONEY\n",
      " and buildings more efficient so that we can save\n",
      "billions of dollars on our energy bills.\n",
      "\n",
      "But to truly transform our \n",
      "Congress = ORG\n",
      "rgy the profitable kind of energy.  So I ask this Congress to\n",
      "send me legislation that places a market-based\n",
      "America = GPE\n",
      "drives the production of more renewable energy in America.  And to support\n",
      "that innovation, we will invest \n",
      "fifteen billion dollars = MONEY\n",
      ".  And to support\n",
      "that innovation, we will invest fifteen billion dollars a year to develop\n",
      "technologies like wind power an\n",
      "America = GPE\n",
      "uel-efficient cars and trucks built right here in America.\n",
      "\n",
      "As for our auto industry, everyone recognizes t\n",
      "that years = DATE\n",
      "a.\n",
      "\n",
      "As for our auto industry, everyone recognizes that years of bad\n",
      "decision-making and a global recession hav\n",
      "Millions = CARDINAL\n",
      "imagined auto\n",
      "industry that can compete and win.  Millions of jobs depend on it.  Scores of\n",
      "communities depe\n",
      "America = GPE\n",
      "e without cost, nor will it be easy.  But this is\n",
      "America.  We don't do what's easy.  We do what is necessa\n",
      "America = GPE\n",
      ".\n",
      "\n",
      "This is a cost that now causes a bankruptcy in America every thirty seconds. \n",
      "By the end of the year, it\n",
      "every thirty seconds = TIME\n",
      "is a cost that now causes a bankruptcy in America every thirty seconds. \n",
      "By the end of the year, it could cause 1.5 mill\n",
      "the end of the year = DATE\n",
      "a bankruptcy in America every thirty seconds. \n",
      "By the end of the year, it could cause 1.5 million Americans to lose the\n",
      "1.5 million = CARDINAL\n",
      " seconds. \n",
      "By the end of the year, it could cause 1.5 million Americans to lose their\n",
      "homes.  In the last eight\n",
      "Americans = NORP\n",
      "y the end of the year, it could cause 1.5 million Americans to lose their\n",
      "homes.  In the last eight years, pr\n",
      "the last eight years = DATE\n",
      "se 1.5 million Americans to lose their\n",
      "homes.  In the last eight years, premiums have grown four times faster than\n",
      "wages\n",
      "four = CARDINAL\n",
      "es.  In the last eight years, premiums have grown four times faster than\n",
      "wages.  And in each of these ye\n",
      "each of these years = DATE\n",
      " have grown four times faster than\n",
      "wages.  And in each of these years, one million more Americans have lost their\n",
      "healt\n",
      "one million = CARDINAL\n",
      "s faster than\n",
      "wages.  And in each of these years, one million more Americans have lost their\n",
      "health insurance. \n",
      "Americans = NORP\n",
      "es.  And in each of these years, one million more Americans have lost their\n",
      "health insurance.  It is one of t\n",
      "last thirty days = DATE\n",
      "to advance the cause of health care reform in the\n",
      "last thirty days than we have in the last decade.  When it was day\n",
      "the last decade = DATE\n",
      "re reform in the\n",
      "last thirty days than we have in the last decade.  When it was days old, this\n",
      "Congress passed a la\n",
      "days = DATE\n",
      "ays than we have in the last decade.  When it was days old, this\n",
      "Congress passed a law to provide and pr\n",
      "Congress = ORG\n",
      "e in the last decade.  When it was days old, this\n",
      "Congress passed a law to provide and protect health insura\n",
      "eleven\n",
      "million = CARDINAL\n",
      "a law to provide and protect health insurance for eleven\n",
      "million American children whose parents work full-time.  \n",
      "American = NORP\n",
      "e and protect health insurance for eleven\n",
      "million American children whose parents work full-time.  Our recov\n",
      "American = NORP\n",
      "disease that has touched the life of nearly every\n",
      "American by seeking a cure for cancer in our time.  And it\n",
      "American = NORP\n",
      "st have quality, affordable health care for every American.  It's a\n",
      "commitment that's paid for in part by ef\n",
      "the years = DATE\n",
      "must take if we hope to bring down our deficit in\n",
      "the years to come.\n",
      "\n",
      "Now, there will be many different opini\n",
      "Democrats = NORP\n",
      "s and workers, doctors\n",
      "and health care providers, Democrats and Republicans to begin work on this\n",
      "issue next \n",
      "Republicans = NORP\n",
      " doctors\n",
      "and health care providers, Democrats and Republicans to begin work on this\n",
      "issue next week.\n",
      "\n",
      "I suffer \n",
      "next week = DATE\n",
      "crats and Republicans to begin work on this\n",
      "issue next week.\n",
      "\n",
      "I suffer no illusions that this will be an easy\n",
      "nearly a century = DATE\n",
      " process.  It will be hard. \n",
      "But I also know that nearly a century after Teddy Roosevelt first called for\n",
      "reform, th\n",
      "Teddy Roosevelt = PERSON\n",
      "ard. \n",
      "But I also know that nearly a century after Teddy Roosevelt first called for\n",
      "reform, the cost of our health c\n",
      "another year = DATE\n",
      "nnot wait, it must not wait, and it will not wait another year.\n",
      "\n",
      "The third challenge we must address is the urge\n",
      "third = ORDINAL\n",
      "not wait, and it will not wait another year.\n",
      "\n",
      "The third challenge we must address is the urgent need to e\n",
      "America = GPE\n",
      "urgent need to expand the promise\n",
      "of education in America.\n",
      "\n",
      "In a global economy where the most valuable ski\n",
      "three-quarters = CARDINAL\n",
      "opportunity -- it\n",
      "is a pre-requisite.\n",
      "\n",
      "Right now, three-quarters of the fastest-growing occupations require more t\n",
      "just over half = CARDINAL\n",
      "equire more than\n",
      "a high school diploma.  And yet, just over half of our citizens have that\n",
      "level of education.  We\n",
      "half = CARDINAL\n",
      " dropout rates of\n",
      "any industrialized nation.  And half of the students who begin college never\n",
      "finish.\n",
      "\n",
      "\n",
      "today = DATE\n",
      ", because we know the countries\n",
      "that out-teach us today will out-compete us tomorrow.  That is why it wil\n",
      "tomorrow = DATE\n",
      "tries\n",
      "that out-teach us today will out-compete us tomorrow.  That is why it will be\n",
      "the goal of this adminis\n",
      "the day = DATE\n",
      "s to a\n",
      "complete and competitive education -- from the day they are born to the day\n",
      "they begin a career.\n",
      "\n",
      "Al\n",
      "those first years = DATE\n",
      "we know that the most formative\n",
      "learning comes in those first years of life.  We have made college affordable\n",
      "for nea\n",
      "nearly seven million = CARDINAL\n",
      "ars of life.  We have made college affordable\n",
      "for nearly seven million more students.  And we have provided the resource\n",
      "tonight = TIME\n",
      "ty of every citizen to participate in it.  And so\n",
      "tonight, I ask every American to commit to at least one y\n",
      "American = NORP\n",
      "o participate in it.  And so\n",
      "tonight, I ask every American to commit to at least one year or more of higher\n",
      "\n",
      "at least one year = DATE\n",
      "And so\n",
      "tonight, I ask every American to commit to at least one year or more of higher\n",
      "education or career training.  \n",
      "four-year = DATE\n",
      "eer training.  This can be community college or a four-year\n",
      "school; vocational training or an apprenticeship.\n",
      "American = NORP\n",
      "iceship.  But whatever the training\n",
      "may be, every American will need to get more than a high school diploma.\n",
      "American = NORP\n",
      "his country needs and values\n",
      "the talents of every American.  That is why we will provide the support\n",
      "necessa\n",
      "2020 = DATE\n",
      " you to complete college and meet a new goal:  by 2020, America\n",
      "will once again have the highest proport\n",
      "America = GPE\n",
      "o complete college and meet a new goal:  by 2020, America\n",
      "will once again have the highest proportion of co\n",
      "Congress = ORG\n",
      "rvice for this and future\n",
      "generations, I ask this Congress to send me the bipartisan legislation that\n",
      "bears \n",
      "Orrin Hatch = PERSON\n",
      "rtisan legislation that\n",
      "bears the name of Senator Orrin Hatch as well as an American who has never\n",
      "stopped aski\n",
      "American = NORP\n",
      "ars the name of Senator Orrin Hatch as well as an American who has never\n",
      "stopped asking what he can do for h\n",
      "Edward Kennedy = PERSON\n",
      " asking what he can do for his country -- Senator Edward Kennedy.\n",
      "\n",
      "These education policies will open the doors of\n",
      "next year = DATE\n",
      "lan free of earmarks, and I want to\n",
      "pass a budget next year that ensures that each dollar we spend reflects o\n",
      "Yesterday = DATE\n",
      "cts only\n",
      "our most important national priorities.\n",
      "\n",
      "Yesterday, I held a fiscal summit where I pledged to cut th\n",
      "half = CARDINAL\n",
      "scal summit where I pledged to cut the deficit in half\n",
      "by the end of my first term in office.  My admini\n",
      "first = ORDINAL\n",
      "edged to cut the deficit in half\n",
      "by the end of my first term in office.  My administration has also begun\n",
      "two trillion dollars = MONEY\n",
      "th the biggest lines.  We have already\n",
      "identified two trillion dollars in savings over the next decade.\n",
      "\n",
      "In this budget,\n",
      "the next decade = DATE\n",
      "y\n",
      "identified two trillion dollars in savings over the next decade.\n",
      "\n",
      "In this budget, we will end education programs \n",
      "billions = CARDINAL\n",
      "l eliminate the\n",
      "no-bid contracts that have wasted billions in Iraq, and reform our defense\n",
      "budget so that we\n",
      "Iraq = GPE\n",
      "the\n",
      "no-bid contracts that have wasted billions in Iraq, and reform our defense\n",
      "budget so that we're not \n",
      "Cold War-era = EVENT\n",
      "m our defense\n",
      "budget so that we're not paying for Cold War-era weapons systems we don't use.\n",
      " We will root out t\n",
      "Medicare = ORG\n",
      " will root out the waste, fraud, and abuse in our Medicare program that\n",
      "doesn't make our seniors any healthi\n",
      "2% = PERCENT\n",
      "e will also end the tax\n",
      "breaks for the wealthiest 2% of Americans.  But let me perfectly clear,\n",
      "becaus\n",
      "Americans = NORP\n",
      " also end the tax\n",
      "breaks for the wealthiest 2% of Americans.  But let me perfectly clear,\n",
      "because I know you'\n",
      "American = NORP\n",
      "se tax\n",
      "breaks means a massive tax increase on the American people:  if your family\n",
      "earns less than $250,000 \n",
      "less than $250,000 = MONEY\n",
      "ase on the American people:  if your family\n",
      "earns less than $250,000 a year, you will not see your taxes increased a\n",
      "s\n",
      "one = CARDINAL\n",
      "our taxes increased a\n",
      "single dime.  I repeat: not one single dime.  In fact, the recovery plan\n",
      "provides\n",
      "95% = PERCENT\n",
      "vides a tax cut -- that's right, a tax cut -- for 95% of working families. \n",
      "And these checks are on the\n",
      "Medicare = ORG\n",
      "health, we must also address the growing\n",
      "costs in Medicare and Social Security.  Comprehensive health care r\n",
      "Social Security = ORG\n",
      "st also address the growing\n",
      "costs in Medicare and Social Security.  Comprehensive health care reform is\n",
      "the best wa\n",
      "Medicare = ORG\n",
      " health care reform is\n",
      "the best way to strengthen Medicare for years to come.  And we must also begin\n",
      "a conv\n",
      "years = DATE\n",
      "reform is\n",
      "the best way to strengthen Medicare for years to come.  And we must also begin\n",
      "a conversation o\n",
      "Social Security = ORG\n",
      "so begin\n",
      "a conversation on how to do the same for Social Security, while creating\n",
      "tax-free universal savings accoun\n",
      "Americans = NORP\n",
      "ating\n",
      "tax-free universal savings accounts for all Americans.\n",
      "\n",
      "Finally, because we're also suffering from a de\n",
      "ten years = DATE\n",
      " our budget.  That is why\n",
      "this budget looks ahead ten years and accounts for spending that was left out\n",
      "under\n",
      "first = ORDINAL\n",
      "t was left out\n",
      "under the old rules -- and for the first time, that includes the full cost of\n",
      "fighting in \n",
      "Iraq = GPE\n",
      " time, that includes the full cost of\n",
      "fighting in Iraq and Afghanistan.  For seven years, we have been a\n",
      "Afghanistan = GPE\n",
      "at includes the full cost of\n",
      "fighting in Iraq and Afghanistan.  For seven years, we have been a nation at\n",
      "war. \n",
      "seven years = DATE\n",
      "ll cost of\n",
      "fighting in Iraq and Afghanistan.  For seven years, we have been a nation at\n",
      "war.  No longer will we\n",
      "Iraq = GPE\n",
      "h wars, and I will soon\n",
      "announce a way forward in Iraq that leaves Iraq to its people and responsibly\n",
      "en\n",
      "Iraq = GPE\n",
      "l soon\n",
      "announce a way forward in Iraq that leaves Iraq to its people and responsibly\n",
      "ends this war.\n",
      "\n",
      "And\n",
      "Afghanistan = GPE\n",
      "e will forge a new and comprehensive\n",
      "strategy for Afghanistan and Pakistan to defeat al Qaeda and combat extrem\n",
      "Pakistan = GPE\n",
      "ew and comprehensive\n",
      "strategy for Afghanistan and Pakistan to defeat al Qaeda and combat extremism.\n",
      " Because\n",
      "al Qaeda = ORG\n",
      "e\n",
      "strategy for Afghanistan and Pakistan to defeat al Qaeda and combat extremism.\n",
      " Because I will not allow t\n",
      "American = NORP\n",
      "e I will not allow terrorists to plot against the American people from\n",
      "safe havens half a world away.\n",
      "\n",
      "As we\n",
      "half = CARDINAL\n",
      "plot against the American people from\n",
      "safe havens half a world away.\n",
      "\n",
      "As we meet here tonight, our men a\n",
      "tonight = TIME\n",
      "m\n",
      "safe havens half a world away.\n",
      "\n",
      "As we meet here tonight, our men and women in uniform stand watch abroad \n",
      "Americans = NORP\n",
      "ilies who bear the quiet burden of their absence, Americans are united in\n",
      "sending one message: we honor your \n",
      "Marines = NORP\n",
      "y\n",
      "budget increases the number of our soldiers and Marines. And to keep our\n",
      "sacred trust with those who serv\n",
      "America = GPE\n",
      "ce in the world more powerful than the\n",
      "example of America. That is why I have ordered the closing of the de\n",
      "Guantanamo Bay = LOC\n",
      "ve ordered the closing of the detention\n",
      "center at Guantanamo Bay, and will seek swift and certain justice for capt\n",
      "tonight = TIME\n",
      "es us stronger.  And that is why I can stand here tonight and\n",
      "say without exception or equivocation that th\n",
      "the United States of America = GPE\n",
      "ht and\n",
      "say without exception or equivocation that the United States of America does\n",
      "not torture.\n",
      "\n",
      "In words and deeds, we are sho\n",
      "America = GPE\n",
      "ew era of engagement has\n",
      "begun.  For we know that America cannot meet the threats of this century\n",
      "alone, bu\n",
      "this century = DATE\n",
      "r we know that America cannot meet the threats of this century\n",
      "alone, but the world cannot meet them without Ame\n",
      "America = GPE\n",
      "ury\n",
      "alone, but the world cannot meet them without America.  We cannot shun the\n",
      "negotiating table, nor ignor\n",
      "Israel = GPE\n",
      "rogress toward a secure and lasting peace between Israel and her\n",
      "neighbors, we have appointed an envoy to \n",
      "the 21st century = DATE\n",
      "to sustain our effort.  To meet the\n",
      "challenges of the 21st century -- from terrorism to nuclear proliferation;\n",
      "from \n",
      "G-20 = PRODUCT\n",
      " in scope, we are working\n",
      "with the nations of the G-20 to restore confidence in our financial system,\n",
      "av\n",
      "American = NORP\n",
      " of escalating protectionism, and spur demand for\n",
      "American goods in markets across the globe.  For the world\n",
      "tonight = TIME\n",
      "aiting for us to lead.\n",
      "\n",
      "Those of us gathered here tonight have been called to govern in extraordinary\n",
      "times\n",
      "Americans = NORP\n",
      "one that has\n",
      "been entrusted to few generations of Americans.  For in our hands lies the\n",
      "ability to shape our \n",
      "Americans = NORP\n",
      "celebrity,\n",
      "but from the dreams and aspirations of Americans who are anything but\n",
      "ordinary.\n",
      "\n",
      "I think about Leo\n",
      "Leonard Abess = PERSON\n",
      "ans who are anything but\n",
      "ordinary.\n",
      "\n",
      "I think about Leonard Abess, the bank president from Miami who reportedly\n",
      "cas\n",
      "Miami = GPE\n",
      "hink about Leonard Abess, the bank president from Miami who reportedly\n",
      "cashed out of his company, took a \n",
      "$60 million = MONEY\n",
      " who reportedly\n",
      "cashed out of his company, took a $60 million bonus, and gave it out to all\n",
      "399 people who work\n",
      "399 = CARDINAL\n",
      " took a $60 million bonus, and gave it out to all\n",
      "399 people who worked for him, plus another 72 who us\n",
      "7 years old = DATE\n",
      "ly said,\n",
      "\"I knew some of these people since I was 7 years old.  I didn't feel right\n",
      "getting the money myself.\"\n",
      "\n",
      "Greensburg = GPE\n",
      "l right\n",
      "getting the money myself.\"\n",
      "\n",
      "I think about Greensburg, Kansas, a town that was completely destroyed by \n",
      "Kansas = GPE\n",
      "ing the money myself.\"\n",
      "\n",
      "I think about Greensburg, Kansas, a town that was completely destroyed by a\n",
      "tornad\n",
      "one = CARDINAL\n",
      "ubble once lay.  \"The tragedy\n",
      "was terrible,\" said one of the men who helped them rebuild.  \"But the fol\n",
      "Ty'Sheoma Bethea = PERSON\n",
      "ed an incredible opportunity.\"\n",
      "\n",
      "And I think about Ty'Sheoma Bethea, the young girl from that school I visited\n",
      "in Dil\n",
      "Dillon = GPE\n",
      "hea, the young girl from that school I visited\n",
      "in Dillon, South Carolina -- a place where the ceilings lea\n",
      "South Carolina = GPE\n",
      " young girl from that school I visited\n",
      "in Dillon, South Carolina -- a place where the ceilings leak, the paint pee\n",
      "six = CARDINAL\n",
      "els\n",
      "off the walls, and they have to stop teaching six times a day because the\n",
      "train barrels by their cl\n",
      "the other day = DATE\n",
      "he has been told that her school is\n",
      "hopeless, but the other day after class she went to the public library and\n",
      "ty\n",
      "South Carolina = GPE\n",
      " so we can make a change to not just the\n",
      "state of South Carolina but also the world.  We are not quitters.\"\n",
      "\n",
      "We ar\n",
      "American = NORP\n",
      "en we will part ways.  But I also know that every\n",
      "American who is sitting here tonight loves this country an\n",
      "tonight = TIME\n",
      "also know that every\n",
      "American who is sitting here tonight loves this country and wants it to\n",
      "succeed.  That\n",
      "the\n",
      "coming months = DATE\n",
      "be the starting point for every debate we have in the\n",
      "coming months, and where we return after those debates are done\n",
      "American = NORP\n",
      "es are done.  That is the\n",
      "foundation on which the American people expect us to build common ground.\n",
      "\n",
      "And if \n",
      "America = GPE\n",
      "of our time and summon\n",
      "that enduring spirit of an America that does not quit, then someday years from\n",
      "now o\n",
      "someday years = DATE\n",
      "ing spirit of an America that does not quit, then someday years from\n",
      "now our children can tell their children tha\n",
      "God Bless = PERSON\n",
      " \"something\n",
      "worthy to be remembered.\"  Thank you, God Bless you, and may God Bless the\n",
      "United States of Ameri\n",
      "God Bless = PERSON\n",
      "e remembered.\"  Thank you, God Bless you, and may God Bless the\n",
      "United States of America.\n",
      "United States of America = GPE\n",
      "  Thank you, God Bless you, and may God Bless the\n",
      "United States of America.\n"
     ]
    }
   ],
   "source": [
    "char_span = 50\n",
    "for ent in ents:\n",
    "    start_char = ent[2] - char_span\n",
    "    if start_char < 0:\n",
    "        start_char = 0\n",
    "    end_char = ent[3] + char_span\n",
    "    if end_char > len(doc_sotu.text):\n",
    "        end_char = len(doc_sotu.text)\n",
    "    print(ent[0], \"=\", ent[1])\n",
    "    print(doc_sotu.text[start_char: end_char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can save only one type of entity. Below, we focus only on person names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Biden',\n",
       " 'Joe',\n",
       " 'Teddy Roosevelt',\n",
       " 'Orrin Hatch',\n",
       " 'Edward Kennedy',\n",
       " 'Leonard Abess',\n",
       " \"Ty'Sheoma Bethea\",\n",
       " 'God Bless',\n",
       " 'God Bless']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_names = []\n",
    "for ent in ents:\n",
    "    if ent[1] == \"PERSON\":\n",
    "        #person_names.append((ent[0], ent[1]))\n",
    "        person_names.append(ent[0])\n",
    "person_names[:10]\n",
    "\n",
    "#list comprehension to produce the same results in one line of code:\n",
    "#person_names = [ent[0] for ent in ents if ent[1] == \"PERSON\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For place names, spaCy offers at least two different types (\"GPE\" and \"LOC\") of place name entities. In the code below, we save both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_names = [(ent[0], ent[1]) for ent in ents if ent[1] in ['GPE', 'LOC']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the United States', 'GPE'),\n",
       " ('the United States of America', 'GPE'),\n",
       " ('Earth', 'LOC'),\n",
       " ('America', 'GPE'),\n",
       " ('Minneapolis', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('Washington', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('the moon', 'LOC'),\n",
       " ('China', 'GPE'),\n",
       " ('Germany', 'GPE'),\n",
       " ('Japan', 'GPE'),\n",
       " ('Korea', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('America', 'GPE'),\n",
       " ('America', 'GPE')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place_names[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a frequency list of the place names found within this address using the Counter method from the **collections** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnfreqs = collections.Counter(place_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('America', 'GPE'): 15,\n",
       "         ('Iraq', 'GPE'): 4,\n",
       "         ('the United States of America', 'GPE'): 2,\n",
       "         ('Afghanistan', 'GPE'): 2,\n",
       "         ('South Carolina', 'GPE'): 2,\n",
       "         ('the United States', 'GPE'): 1,\n",
       "         ('Earth', 'LOC'): 1,\n",
       "         ('Minneapolis', 'GPE'): 1,\n",
       "         ('Washington', 'GPE'): 1,\n",
       "         ('the moon', 'LOC'): 1,\n",
       "         ('China', 'GPE'): 1,\n",
       "         ('Germany', 'GPE'): 1,\n",
       "         ('Japan', 'GPE'): 1,\n",
       "         ('Korea', 'GPE'): 1,\n",
       "         ('Pakistan', 'GPE'): 1,\n",
       "         ('Guantanamo Bay', 'LOC'): 1,\n",
       "         ('Israel', 'GPE'): 1,\n",
       "         ('Miami', 'GPE'): 1,\n",
       "         ('Greensburg', 'GPE'): 1,\n",
       "         ('Kansas', 'GPE'): 1,\n",
       "         ('Dillon', 'GPE'): 1,\n",
       "         ('United States of America', 'GPE'): 1})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnfreqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX. Extracting Named Entities from the entire corpus\n",
    "\n",
    "We can now scale up and extract NEs from the entire SOTU corpus. A researcher interested in identifying and mapping locations mentioned in these addresses, for example, may want to export all place names. They could extract both \"GPEs\" (geopolitical entities like countries) and \"LOC\" (other types of place names). But, for the example below, we will focus on GPEs, which are easier to map.\n",
    "\n",
    "First, we will create a function that extracts all GPEs from a given text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_placenames (text):\n",
    "    doc = nlp(text)\n",
    "    ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]\n",
    "    #place_names = [(ent[0], ent[1]) for ent in ents if ent[1] in ['GPE', 'LOC']]\n",
    "    gpes = [ent[0] for ent in ents if ent[1] == \"GPE\"]\n",
    "    return(gpes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can apply this function to all our texts found in our sotu dataframe. The code below creates a new column called \"gpes\" that stores a list of the GPEs found within each text.\n",
    "\n",
    "*Note: This will take several minutes to run across the SOTU corpus of 233 SOTU addresses. It took 5 minutes to complete on my relatively fast laptop*. The apply function is commented out, but when you have 5 - 20 minutes to spare feel free to run it and check out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotudf['gpes'] = sotudf['fulltext'].apply(extract_placenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have created the new \"gpes\" column with the code above, you may also uncomment out the code below (remove the `\"\"\"`) to create one long list of all gpes found within the SOTU corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_gpes = list([a for b in sotudf.gpes.tolist() for a in b])\n",
    "all_gpes = [gpe.replace(\"\\n\", \" \") for gpe in all_gpes]\n",
    "print(len(all_gpes))\n",
    "\n",
    "with open('sotudf_gpes.txt', 'w') as f:\n",
    "    for gpe in all_gpes:\n",
    "        f.write(f\"{gpe}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a frequency list of these GPEs, which could be used in subsequent efforts to create a map of all countries in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the United States', 3478),\n",
       " ('America', 1418),\n",
       " ('States', 1187),\n",
       " ('Mexico', 680),\n",
       " ('United States', 537),\n",
       " ('Spain', 454),\n",
       " ('Great Britain', 446),\n",
       " ('China', 347),\n",
       " ('France', 338),\n",
       " ('Washington', 323),\n",
       " ('Cuba', 308),\n",
       " ('Texas', 249),\n",
       " ('Japan', 222),\n",
       " ('Russia', 190),\n",
       " ('Germany', 153),\n",
       " ('California', 145),\n",
       " ('Nicaragua', 136),\n",
       " ('Iraq', 129),\n",
       " ('The United States', 126),\n",
       " ('New York', 118)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_gpes_freqs = collections.Counter(all_gpes)\n",
    "sotu_gpes_freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may observe some problems here. \"The United States\", \"the United States\", \"United States\", and \"America\" are considered separated entities. We would want to aggregate them into one entity. We also have a few state and/or city names mixed in with the country names. More data cleaning is necessary. But, it is a good starting point...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
